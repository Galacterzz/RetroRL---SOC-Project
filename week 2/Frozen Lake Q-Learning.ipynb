{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cf8c45f-cc18-4a09-ab1b-87280a97ce4d",
   "metadata": {},
   "source": [
    "# Frozen Lake Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e670d0c-9ad5-4c7e-a478-da9566f88a4b",
   "metadata": {},
   "source": [
    "Q-learning is a model-free reinforcement learning algorithm used to find the optimal action-selection policy for a given environment. It learns by iteratively updating estimates of the optimal action-value function $Qâˆ—(s,a)$, which represents the expected cumulative future rewards of taking action aa in state ss and then following the optimal policy thereafter.\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d59316e-44ea-490f-a4bb-cf86586ecab7",
   "metadata": {},
   "source": [
    "Why I choose Frozen lake environment ?\n",
    "- I choose Frozen lake because, Q-Learning works when the observation state has discrete values rather than continous.\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902e1b3d-69fa-46af-b080-dbf9a72e8792",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e66e2f7-a9f3-4e90-87ac-f9c2b4866b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d691ed4d-add0-415f-9416-ca71d6574f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d894da8-ec0b-40b3-998e-60e1baf70acb",
   "metadata": {},
   "source": [
    "## Declaring Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c972cdfb-01ad-4a8c-aa4d-b36a1dda49a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "alpha = 0.1  # Learning rate\n",
    "gamma = 0.9  # Discount factor\n",
    "epsilon = 1.0  # Exploration rate (decays over time)\n",
    "max_epsilon = 1.0  # Initial exploration rate\n",
    "min_epsilon = 0.01  # Minimum exploration rate\n",
    "decay_rate = 0.01  # Decay rate for epsilon\n",
    "num_episodes = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87a0860-032f-4182-93d4-190d87ee8166",
   "metadata": {},
   "source": [
    "## Making the Q-Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76d41136-92c6-4c08-bb59-4e664405cb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-table initialization\n",
    "Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "# Reward per episode record\n",
    "rewards_per_episode = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a543b361-2b15-4970-bede-32a1931eb91b",
   "metadata": {},
   "source": [
    "## Running the episode to learn and update Q Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1aca0e83-d0d2-4c1c-bfdb-ad284356ec34",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Update Q-table using Bellman equation for Q-learning\u001b[39;00m\n\u001b[0;32m     21\u001b[0m expected_future_reward \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(Q[next_state, :]) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 22\u001b[0m \u001b[43mQ\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m alpha \u001b[38;5;241m*\u001b[39m (reward \u001b[38;5;241m+\u001b[39m gamma \u001b[38;5;241m*\u001b[39m expected_future_reward \u001b[38;5;241m-\u001b[39m Q[state[\u001b[38;5;241m0\u001b[39m], action])\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Update total reward for the episode\u001b[39;00m\n\u001b[0;32m     25\u001b[0m total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "\u001b[1;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "for episode in range(num_episodes):\n",
    "  # Reset environment for each episode\n",
    "  state = env.reset()\n",
    "  done = False\n",
    "  total_reward = 0\n",
    "\n",
    "  # Epsilon decay for exploration vs exploitation balance\n",
    "  epsilon = max_epsilon - (max_epsilon - min_epsilon) * episode / num_episodes\n",
    "\n",
    "  while not done:\n",
    "    # Choose action based on epsilon-greedy strategy\n",
    "    if np.random.rand() < epsilon:\n",
    "      action = env.action_space.sample()  # Explore random action\n",
    "    else:\n",
    "      action = np.argmax(Q[state, :])  # Exploit best known action\n",
    "\n",
    "    # Take action, observe next state, reward, and done flag\n",
    "    next_state, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "    # Update Q-table using Bellman equation for Q-learning\n",
    "    expected_future_reward = np.max(Q[next_state, :]) if not done else 0\n",
    "    Q[state, action] += alpha * (reward + gamma * expected_future_reward - Q[state[0], action])\n",
    "\n",
    "    # Update total reward for the episode\n",
    "    total_reward += reward\n",
    "\n",
    "    state = next_state\n",
    "\n",
    "  # Append the total reward for the episode\n",
    "  rewards_per_episode.append(total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24661d4-2063-4ed3-9a6e-bbe8874fc9be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
